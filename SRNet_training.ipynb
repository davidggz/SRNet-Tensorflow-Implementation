{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dutch-definition",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import sys\n",
    "import random\n",
    "import os\n",
    "import h5py\n",
    "from sklearn.metrics import  confusion_matrix, accuracy_score, roc_curve, auc\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib\n",
    "from statsmodels.stats.proportion import proportion_confint\n",
    "import matplotlib.ticker as mticker\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Tensorflow imports\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import callbacks\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Get the important functions from the other files\n",
    "from paired_image_generator import PairedImageGenerator\n",
    "from SRNet_model import create_SRNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "veterinary-breeding",
   "metadata": {},
   "source": [
    "# Experiment constants"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unknown-stream",
   "metadata": {},
   "source": [
    "These constants are practically all the constants that govern the experiment. In general, most of them are related to the parameters of the Keras **flow_from_directory** function. ([Documentation of the function](https://keras.io/api/preprocessing/image/)):\n",
    "\n",
    "- **EXPERIMENT_NAME:** Name of this experiment. This is the name that will be used to create the output files like the model results folder or the different figures.\n",
    "\n",
    "\n",
    "- **NUM_EPOCHS:** Number of times all the images of the train set are used in the network to train.\n",
    "\n",
    "\n",
    "- **BATCH_SIZE**: Number of images to train in each minibatch.\n",
    "\n",
    "\n",
    "- **CLASS_MODE:** It has two possible values: **'categorical'** y **'binary'**. This only affects the validation generator.\n",
    "    - **'categorical':** The labels are encoded with One Hot Encoding.\n",
    "    - **'binary':** The labels are encoded with 0 and 1.\n",
    "\n",
    "\n",
    "- **CLASSES:** This is the variable that set which are the classes that are going to be used during the training process. It's important in order to know the order of the output images.\n",
    "\n",
    "\n",
    "- **INPUT_IMAGE_SIZE:** Size that all the images will be rescaled to just before going into the neural network.\n",
    "\n",
    "\n",
    "- **COLOR_MODE:** This variable set the number of channels that we want to use in the inputted images. The possible values are: 'rgb', 'grayscale' and 'rgba'. **We will use 'grayscale' that just have one channel.**\n",
    "\n",
    "\n",
    "- **THRESHOLD:** Threshold that will be used to build the confusion matrix and the Confidence Interval for the validation accuracy.\n",
    "\n",
    "\n",
    "- **SEED.** This seed is trying to ensure the replicability of the experiments. However, Tensorflow doesn't work properly with seeds as far as I know so experiments are not correctly replicated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collect-telling",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directories\n",
    "MODELS_DIR = 'trained_models'\n",
    "BASE_DIR  = 'dataset/SRNet-Dataset-0.4'\n",
    "TRAIN_DIR = os.path.join(BASE_DIR, 'train')\n",
    "VAL_DIR   = os.path.join(BASE_DIR, 'val')\n",
    "TEST_DIR  = os.path.join(BASE_DIR, 'test')\n",
    "\n",
    "# Constants\n",
    "EXPERIMENT_NAME = 'SRNet-experiment'\n",
    "NUM_EPOCHS = 200\n",
    "BATCH_SIZE = 16\n",
    "CLASS_MODE  = 'binary'\n",
    "CLASSES = ['0', '1'] # 0 -> Cover, 1 -> Stego\n",
    "INPUT_IMAGE_SIZE = (256, 256, 1)\n",
    "COLOR_MODE  = 'grayscale'\n",
    "THRESHOLD = 0.5\n",
    "SEED = 483\n",
    "\n",
    "# Number of train, validation and test images\n",
    "NUM_TRAIN = sum([len(files) for _, _, files in os.walk(TRAIN_DIR)])\n",
    "NUM_VAL = sum([len(files) for _, _, files in os.walk(VAL_DIR)])\n",
    "NUM_TEST = sum([len(files) for _, _, files in os.walk(TEST_DIR)])\n",
    "\n",
    "# This parameter should be None always unless the pipeline is being tested.\n",
    "# It only forces the epochs to be very short for testing.\n",
    "STEPS_PER_EPOCH = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11aa55c8-c6d0-417e-a0fa-1b6b585033e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set all the possible seeds to be the same one.\n",
    "tf.random.set_seed(SEED)\n",
    "np.random.seed(seed=SEED)\n",
    "random.seed(SEED)\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9168f65-5af3-466c-950c-40238ee817c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check all the available GPUs\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "  tf.config.experimental.set_memory_growth(gpu, True)\n",
    "\n",
    "print(gpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "living-taiwan",
   "metadata": {},
   "source": [
    "# Image generator creation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attended-africa",
   "metadata": {},
   "source": [
    "I've decided to use image generators because that's a good way to manage a lot of images easily.\n",
    "\n",
    "- **ImageDataGenerator:** When calling this function we can specify different changes to be made to the images. For example, we can normalize all the images. [Here](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator) we can see all the different modifications that are available for data augmentation.\n",
    "\n",
    "\n",
    "- **flow_from_directory:** Function that is used to get the generator itself. It has several parameters that are useful for the training process. In the first text cell above we can see the most important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a400c7-09eb-4130-adbb-f62f1eb5811a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the images\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Select where to obtain the images from and some parameters\n",
    "val_generator = val_datagen.flow_from_directory(VAL_DIR,\n",
    "                                                target_size=INPUT_IMAGE_SIZE[0:2],\n",
    "                                                batch_size=BATCH_SIZE,\n",
    "                                                class_mode=CLASS_MODE,\n",
    "                                                classes=CLASSES,\n",
    "                                                color_mode=COLOR_MODE,\n",
    "                                                shuffle=False,\n",
    "                                                seed=SEED)\n",
    "\n",
    "# Use the custom image generator present in the paired_image_generator.py file.\n",
    "# The data augmentation of the images is within this generator.\n",
    "train_generator = PairedImageGenerator(dim = (256, 256),\n",
    "                                       n_channels = INPUT_IMAGE_SIZE[2], \n",
    "                                       batch_size = BATCH_SIZE, \n",
    "                                       images_path_cover = os.path.join(TRAIN_DIR, '0'), \n",
    "                                       images_path_stego = os.path.join(TRAIN_DIR, '1'), \n",
    "                                       shuffle = True,\n",
    "                                       augment = True,\n",
    "                                       seed = SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4338d4-ff45-4e2e-9b62-12944a96a4ba",
   "metadata": {},
   "source": [
    "In the following cell, a batch is retrieved from the generator and four images of it are shown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d66d22ac-d8a0-4dea-a504-ed22b4d4e940",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = train_generator.__getitem__(1242)\n",
    "\n",
    "first_images = batch[0][0:4]\n",
    "\n",
    "# Plot of several images to check how they enter the network\n",
    "for np_image in first_images:\n",
    "    plt.imshow(np.reshape(np_image, (256, 256)), interpolation='none', cmap='gray')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hollywood-implementation",
   "metadata": {
    "id": "comparative-while",
    "tags": []
   },
   "source": [
    "# Model creation\n",
    "\n",
    "The architecture follows the guidelines given in [\"Deep Residual Network for Steganalysis of Digital Images\" by Mehdi Boroumand et al.](https://ieeexplore.ieee.org/document/8470101). The only difference between this implementation and the one introduced in the paper is the output activation function. In this implementation a Sigmoid is used in order to make threshold analysis with the model. However, the paper uses a Softmax output activation function with two output neurons.\n",
    "\n",
    "The `create_SRNet` function comes from the `SRNet_model.py` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "received-baking",
   "metadata": {
    "id": "future-county"
   },
   "outputs": [],
   "source": [
    "# Create the model\n",
    "model = create_SRNet(INPUT_IMAGE_SIZE)\n",
    "\n",
    "# Compile the model selecting the loss, the optimizer and the metrics.\n",
    "model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),\n",
    "              optimizer=optimizers.Adamax(learning_rate=0.001),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sharp-merchant",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The training directory is created\n",
    "try:\n",
    "    os.mkdir(os.path.join(MODELS_DIR, EXPERIMENT_NAME))\n",
    "except OSError:\n",
    "    pass\n",
    "\n",
    "# The directory with the checkpoints is created\n",
    "try:\n",
    "    os.mkdir(os.path.join(MODELS_DIR, EXPERIMENT_NAME, 'checkpoints'))\n",
    "except OSError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "boxed-discipline",
   "metadata": {
    "id": "arctic-grounds",
    "outputId": "55b93f59-e94e-4c6f-adc1-8072af9943e6"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Callback to stop the algorithm when it doesn't improve.\n",
    "early_stopping = callbacks.EarlyStopping(monitor='val_accuracy', \n",
    "                                        min_delta=0, \n",
    "                                        patience=100, \n",
    "                                        verbose=0, \n",
    "                                        mode='max', \n",
    "                                        baseline=None, \n",
    "                                        restore_best_weights=True)\n",
    "\n",
    "# Callback to continuously save the best model after every epoch.\n",
    "model_checkpoint = callbacks.ModelCheckpoint(os.path.join(MODELS_DIR, EXPERIMENT_NAME, 'checkpoints', 'best_model.h5'), \n",
    "                                             monitor='val_accuracy', \n",
    "                                             verbose=0, \n",
    "                                             save_best_only=False,\n",
    "                                             save_weights_only=False, \n",
    "                                             mode='max', \n",
    "                                             save_freq='epoch')\n",
    "\n",
    "# Callback to change the learning rate after 150 epochs\n",
    "def lr_schedule(epoch):\n",
    "    if epoch <= 149:\n",
    "        return 0.001\n",
    "    else:\n",
    "        return 0.0001\n",
    "\n",
    "learning_rate_scheduler = callbacks.LearningRateScheduler(lr_schedule, verbose=0)\n",
    "\n",
    "# This callback saves the whole train history as a csv file\n",
    "csv_logger = callbacks.CSVLogger(os.path.join(MODELS_DIR, EXPERIMENT_NAME, 'training_log.csv'), append=True, separator=';')\n",
    "\n",
    "\n",
    "# Execute the training with all the callbacks\n",
    "trainHistory = model.fit(train_generator,\n",
    "                         steps_per_epoch=NUM_TRAIN//BATCH_SIZE if STEPS_PER_EPOCH == None else STEPS_PER_EPOCH,\n",
    "                         epochs=NUM_EPOCHS, \n",
    "                         validation_data=val_generator,\n",
    "                         validation_steps=NUM_VAL//BATCH_SIZE if STEPS_PER_EPOCH == None else STEPS_PER_EPOCH,\n",
    "                         callbacks=[csv_logger, early_stopping, model_checkpoint, learning_rate_scheduler])\n",
    "\n",
    "# We load the best model obtained during validation\n",
    "best_model = load_model(os.path.join(MODELS_DIR, EXPERIMENT_NAME, 'checkpoints', 'best_model.h5'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "knowing-arrest",
   "metadata": {},
   "source": [
    "# Evaluation graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eee9500-4dc3-440d-98e7-691a4c415c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_line_plot(df_training_log, metric_name, ax, tick_size):\n",
    "    # Plot train trend\n",
    "    sns.lineplot(x=range(1, len(df_training_log) + 1), \n",
    "                 y=df_training_log[metric_name], \n",
    "                 ax=ax, \n",
    "                 label=\"Train\", \n",
    "                 palette=\"tab10\")\n",
    "    \n",
    "    # Plot validation train\n",
    "    sns.lineplot(x=range(1, len(df_training_log) + 1), \n",
    "                 y=df_training_log[f'val_{metric_name}'], \n",
    "                 ax=ax, \n",
    "                 label=\"Validation\", \n",
    "                 palette=\"tab10\")\n",
    "\n",
    "    # Add the legend\n",
    "    ax.legend(fontsize=14)\n",
    "\n",
    "    # Change axis labels\n",
    "    ax.set_ylabel(metric_name.capitalize(), fontdict={'fontsize':tick_size + 1})\n",
    "\n",
    "    # Set the size of the y ticks\n",
    "    ticks_loc = ax.get_yticks().tolist()\n",
    "    ax.yaxis.set_major_locator(mticker.FixedLocator(ticks_loc))\n",
    "    ax.set_yticklabels(['{:.3f}'.format(value) for value in ticks_loc], fontsize = tick_size)\n",
    "    \n",
    "\n",
    "def gen_trend_plot(df_training_log):\n",
    "    sns.set(style=\"darkgrid\")\n",
    "    \n",
    "    # Generate the general plot\n",
    "    fig, axes = plt.subplots(2, 1, \n",
    "                             figsize=(10, 8),\n",
    "                             sharex=True)\n",
    "    \n",
    "    # Font size\n",
    "    tick_size = 17\n",
    "\n",
    "    # Set the general features of the plot\n",
    "    plt.xlabel('Epoch', fontdict={'fontsize':tick_size + 1})\n",
    "    plt.tick_params(axis='x', which='major', labelsize=tick_size)\n",
    "    fig.suptitle(EXPERIMENT_NAME + \" training trends\", fontsize=21)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Generate both subplots\n",
    "    gen_line_plot(df_training_log, 'accuracy', axes[0], tick_size)\n",
    "    gen_line_plot(df_training_log, 'loss', axes[1], tick_size)\n",
    "    \n",
    "    # Save the trend plot\n",
    "    plt.savefig(os.path.join(MODELS_DIR, EXPERIMENT_NAME, EXPERIMENT_NAME + '_training_trend.pdf'), bbox_inches='tight')\n",
    "    \n",
    "\n",
    "def get_confidence_interval(success_rate, n, alpha):\n",
    "    # Get the confidence interval of the prediction\n",
    "    confidence_interval_tuple = proportion_confint(n*success_rate, n, method='wilson', alpha=alpha)\n",
    "    \n",
    "    # Reformat the tuple to have less decimal numbers\n",
    "    return tuple([float(\"{0:.4f}\".format(limit)) for limit in confidence_interval_tuple])\n",
    "\n",
    "\n",
    "def gen_confusion_matrix(predictions, generator, num_images, threshold=0.5, alpha=0.05):\n",
    "    sns.set_style(\"darkgrid\")\n",
    "    matplotlib.rc('xtick', labelsize=15) \n",
    "    matplotlib.rc('ytick', labelsize=15) \n",
    "    \n",
    "    # Get the discrete predictions out of the continuous output\n",
    "    discrete_predicted_labels = np.round(predictions - threshold + 0.5)\n",
    "    real_labels = generator.classes\n",
    "\n",
    "    # Get the raw confusion matrix in numpy format\n",
    "    conf_matrix = confusion_matrix(real_labels, \n",
    "                                   discrete_predicted_labels, \n",
    "                                   labels = list(map(lambda x: generator.class_indices[x], CLASSES)))\n",
    "\n",
    "    # Transform the confusion matrix into a DataFrame\n",
    "    conf_matrix = pd.DataFrame(conf_matrix, columns = CLASSES, index = CLASSES)\n",
    "    \n",
    "    # Get the accuracy from the predictions\n",
    "    accuracy = accuracy_score(real_labels, discrete_predicted_labels)\n",
    "    \n",
    "    # Generate the confussion matrix\n",
    "    fig, ax = plt.subplots(figsize=(5, 4))\n",
    "    sns.set(font_scale=2)\n",
    "    sns.heatmap(conf_matrix, annot=True, cmap='Blues', fmt='g', ax=ax, cbar=False)\n",
    "\n",
    "    # Labels, title and ticks\n",
    "    label_font = {'size':'16'}\n",
    "    ax.set_xlabel('Predicted value', fontdict=label_font)\n",
    "    ax.set_ylabel('True value', fontdict=label_font)\n",
    "    \n",
    "    # Title\n",
    "    title_font = {'size':'17'}\n",
    "    ax.set_title(EXPERIMENT_NAME + \n",
    "                 ' Confusion matrix' + \n",
    "                 '\\nValidation accuracy: {0:.3f}'.format(accuracy) + \n",
    "                 f'\\nConfidence Interval ({alpha}): {str(get_confidence_interval(accuracy, conf_matrix.sum().sum(), alpha)[0])}', \n",
    "                 fontdict=title_font)\n",
    "\n",
    "    plt.savefig(os.path.join(MODELS_DIR, EXPERIMENT_NAME, EXPERIMENT_NAME + '_confusion_matrix.pdf'), bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e849e8af-7cc8-4573-99c1-8d9d9d6df559",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels = best_model.predict(val_generator, verbose = 0).ravel()\n",
    "gen_trend_plot(trainHistory.history)\n",
    "gen_confusion_matrix(predicted_labels, val_generator, num_images=NUM_VAL, threshold=THRESHOLD, alpha=0.05)\n",
    "\n",
    "# Save the prediction in the validation set\n",
    "with open(os.path.join(MODELS_DIR, EXPERIMENT_NAME, f'{EXPERIMENT_NAME}_raw_prediction.npy'), 'wb') as f:\n",
    "    np.save(f, predicted_labels)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "training.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
